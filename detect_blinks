#!/usr/bin/python3
"""
USAGE:
detect_blinks
detect_blinks -v "path/to/video/file.mp4"
"""
import argparse
import time
import imutils
import numpy as np
import cv2
import dlib
from scipy.spatial import distance as dist
from imutils.video import FileVideoStream
from imutils.video import VideoStream
from imutils import face_utils


def eye_aspect_ratio(eye):
    """
    Compute the ratio of the euclidian distances between two sets
    of vertical and between horizontal eye landmarks
    """
    vert1 = dist.euclidean(eye[1], eye[5])
    vert2 = dist.euclidean(eye[2], eye[4])
    hor = dist.euclidean(eye[0], eye[3])
    return (vert1 + vert2) / (2.0 * hor)


if __name__ == '__main__':
    webcam_fps = 30
    FPS = None
    # Parse arguments, set corresponding streaming mode
    ap = argparse.ArgumentParser()
    ap.add_argument("-v", "--video", type=str, default="",
                    help="path to input video file")
    args = vars(ap.parse_args())
    vs, file_stream = None, None
    file_stream = not args["video"] == ""
    if file_stream:
        print("[INFO] Using video from file")
        video = FileVideoStream(args["video"])
        FPS = round(video.stream.get(cv2.CAP_PROP_FPS))
        vs = video.start()
    else:
        print("[INFO] Using web camera")
        FPS = webcam_fps
        vs = VideoStream(src=0, framerate=FPS).start()

    print("[INFO] FPS: ", FPS)

    # Initialize dlib's face detector (HOG-based) and then create
    # the facial landmark predictor
    print("[INFO] loading facial landmark predictor...")
    detector = dlib.get_frontal_face_detector()
    predictor = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")

    # Grab the indexes of the facial landmarks for the left and
    # right eye, respectively
    (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS["left_eye"]
    (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS["right_eye"]

    # Initialize parameters
    SETUP_FRAMES = FPS * 3
    EYE_AR_CONSEC_FRAMES = 3
    EYE_AR_SLEEP_FRAMES = FPS * 2

    # To be tuned
    EYE_AR_THRESH = None

    # Frame counters for blinks
    COUNTER = 0
    TOTAL = 0
    n_frames = 0

    # Vector for EYE_AR_THRESH calculation
    ear_vec = np.empty((0,))

    frame = imutils.resize(vs.read(), width=450)
    out = cv2.VideoWriter("/out/out.mp4",
                          cv2.VideoWriter_fourcc(*"mp4v"),
                          FPS, frame.shape[1::-1])

    # Loop over frames from video stream
    while True:
        # Grab the frame from the threaded video file stream, 
        # resize it, and convert it to grayscale
        if frame is None:
            break
        frame = imutils.resize(frame, width=450)
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Detect faces in the grayscale frame
        rects = detector(gray, 0)

        # Determine the facial landmarks for the face region,
        # then convert the facial landmark (x, y)-coordinates
        # to a numpy array
        for rect in rects: 
            shape = predictor(gray, rect)
            shape = face_utils.shape_to_np(shape)

            # Extract the left and right eye coordinates, then 
            # use the coordinates to compute the eye aspect 
            # ratio for both eyes
            leftEye = shape[lStart:lEnd]
            rightEye = shape[rStart:rEnd]
            leftEAR = eye_aspect_ratio(leftEye)
            rightEAR = eye_aspect_ratio(rightEye)

            # Average the eye aspect ratio together for both eyes
            ear = (leftEAR + rightEAR) / 2.0

            # Compute the convex hull 
            # then visualize each of the eyes
            leftEyeHull = cv2.convexHull(leftEye)
            rightEyeHull = cv2.convexHull(rightEye)
            cv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)
            cv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)

            if n_frames < SETUP_FRAMES:
                ear_vec = np.append(ear_vec, ear)
                cv2.putText(frame, "Learning your eyes...", (10, 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                n_frames += 1
            elif n_frames == SETUP_FRAMES:
                ear_vec = np.append(ear_vec, ear)
                mean = np.sum(ear_vec) / ear_vec.shape[0]
                var = np.cov(ear_vec)
                EYE_AR_THRESH = mean - np.sqrt(var)
                n_frames += 1
            else:       
                # Check to see if the eye aspect ratio is below the blink
                # threshold, and if so, increment the blink frame counter
                if ear < EYE_AR_THRESH:
                    COUNTER += 1
                    if COUNTER >= EYE_AR_SLEEP_FRAMES:
                        cv2.putText(frame, "SLEEPING ALERT", (10, 160),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

                # Otherwise, the eye aspect ratio is not below the blink
                # threshold
                else:
                    if COUNTER >= EYE_AR_CONSEC_FRAMES:
                        TOTAL += 1
                    COUNTER = 0

                # Draw the total number of blinks on the frame
                cv2.putText(frame, "Setup is complete", (10, 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                cv2.putText(frame, "Blinks: {}".format(TOTAL), (10, 100),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)                

            # Draw the current EAR on the frame
            cv2.putText(frame, "EAR: {:.2f}".format(ear), (300, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

        cv2.imshow("", frame)
        out.write(frame)

        key = cv2.waitKey(1) & 0xFF
        if key == ord("q"):
            break

        frame = vs.read()

    out.release()
    cv2.destroyAllWindows()
    vs.stop()
